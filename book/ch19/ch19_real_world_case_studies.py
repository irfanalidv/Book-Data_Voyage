#!/usr/bin/env python3
"""
Chapter 19: Real-World Case Studies
===================================

This chapter demonstrates practical applications of data science across
various industries including e-commerce, healthcare, finance, and marketing.

Topics Covered:
- E-Commerce Customer Analytics
- Healthcare Data Science
- Financial Analytics and Risk Management
- Marketing and Customer Intelligence
- Supply Chain and Operations
"""

import os
import sys
import time
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from typing import Dict, List, Any, Tuple

# Machine Learning imports
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, LogisticRegression

# Set up plotting style
plt.style.use("default")
sns.set_palette("husl")
warnings.filterwarnings("ignore")

# Set random seed for reproducibility
np.random.seed(42)


class ECommerceCaseStudy:
    """E-Commerce customer analytics case study."""

    def __init__(self):
        self.customer_data = None
        self.transaction_data = None
        self.segments = None

    def create_ecommerce_data(self):
        """Create synthetic e-commerce dataset."""
        print("1. E-COMMERCE CUSTOMER ANALYTICS")
        print("=" * 50)

        print("\n1.1 CREATING SYNTHETIC E-COMMERCE DATASET:")
        print("-" * 45)

        # Generate customer data
        n_customers = 1000

        customer_data = {
            "customer_id": range(1, n_customers + 1),
            "age": np.random.normal(35, 12, n_customers).astype(int),
            "income": np.random.lognormal(10.5, 0.5, n_customers).astype(int),
            "total_purchases": np.random.poisson(15, n_customers),
            "avg_order_value": np.random.lognormal(4, 0.3, n_customers),
            "days_since_last_purchase": np.random.exponential(30, n_customers).astype(
                int
            ),
            "customer_satisfaction": np.random.uniform(1, 5, n_customers),
            "loyalty_program": np.random.choice(
                [True, False], n_customers, p=[0.7, 0.3]
            ),
        }

        self.customer_data = pd.DataFrame(customer_data)

        # Generate transaction data
        n_transactions = 5000
        transaction_data = {
            "transaction_id": range(1, n_transactions + 1),
            "customer_id": np.random.choice(range(1, n_customers + 1), n_transactions),
            "product_category": np.random.choice(
                ["Electronics", "Clothing", "Books", "Home", "Sports"], n_transactions
            ),
            "amount": np.random.lognormal(3.5, 0.8, n_transactions),
            "date": pd.date_range("2023-01-01", periods=n_transactions, freq="H"),
            "payment_method": np.random.choice(
                ["Credit Card", "PayPal", "Bank Transfer"], n_transactions
            ),
            "is_returned": np.random.choice(
                [True, False], n_transactions, p=[0.1, 0.9]
            ),
        }

        self.transaction_data = pd.DataFrame(transaction_data)

        print(f"  ‚úÖ Customer dataset: {len(self.customer_data):,} customers")
        print(f"  ‚úÖ Transaction dataset: {len(self.transaction_data):,} transactions")
        print(
            f"  üìä Data spans: {self.transaction_data['date'].min()} to {self.transaction_data['date'].max()}"
        )

        return self.customer_data, self.transaction_data

    def customer_segmentation(self):
        """Perform customer segmentation analysis."""
        print("\n1.2 CUSTOMER SEGMENTATION ANALYSIS:")
        print("-" * 40)

        # Prepare features for clustering
        features = [
            "age",
            "income",
            "total_purchases",
            "avg_order_value",
            "days_since_last_purchase",
        ]
        X = self.customer_data[features].copy()

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Perform K-means clustering
        kmeans = KMeans(n_clusters=4, random_state=42)
        self.customer_data["segment"] = kmeans.fit_predict(X_scaled)

        # Analyze segments
        segment_analysis = self.customer_data.groupby("segment")[features].mean()

        print("  üîç Customer Segments Identified:")
        for segment in range(4):
            segment_size = len(
                self.customer_data[self.customer_data["segment"] == segment]
            )
            segment_pct = (segment_size / len(self.customer_data)) * 100

            print(
                f"    Segment {segment}: {segment_size} customers ({segment_pct:.1f}%)"
            )

        print("\n  üìä Segment Characteristics:")
        print(segment_analysis.round(2))

        # Assign meaningful names to segments
        segment_names = {
            0: "High-Value Loyal",
            1: "Budget Conscious",
            2: "Occasional Buyers",
            3: "Premium Customers",
        }

        self.customer_data["segment_name"] = self.customer_data["segment"].map(
            segment_names
        )

        return self.customer_data

    def churn_prediction(self):
        """Build churn prediction model."""
        print("\n1.3 CHURN PREDICTION MODEL:")
        print("-" * 35)

        # Create churn target (customers who haven't purchased in 60+ days)
        self.customer_data["is_churned"] = (
            self.customer_data["days_since_last_purchase"] > 60
        ).astype(int)

        # Prepare features for churn prediction
        churn_features = [
            "age",
            "income",
            "total_purchases",
            "avg_order_value",
            "customer_satisfaction",
            "loyalty_program",
        ]
        X = self.customer_data[churn_features].copy()

        # Encode categorical variables
        X["loyalty_program"] = X["loyalty_program"].astype(int)
        y = self.customer_data["is_churned"]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # Train churn prediction model
        churn_model = RandomForestClassifier(n_estimators=100, random_state=42)
        churn_model.fit(X_train, y_train)

        # Evaluate model
        y_pred = churn_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, churn_model.predict_proba(X_test)[:, 1])

        print(f"  üéØ Churn Prediction Results:")
        print(f"    Accuracy: {accuracy:.4f}")
        print(f"    AUC: {auc:.4f}")
        print(f"    Churn Rate: {y.mean():.1%}")

        # Feature importance
        feature_importance = pd.DataFrame(
            {"feature": churn_features, "importance": churn_model.feature_importances_}
        ).sort_values("importance", ascending=False)

        print(f"\n  üîç Top Features for Churn Prediction:")
        for _, row in feature_importance.head(3).iterrows():
            print(f"    {row['feature']:20}: {row['importance']:.4f}")

        return churn_model, feature_importance


class HealthcareCaseStudy:
    """Healthcare data science case study."""

    def __init__(self):
        self.patient_data = None
        self.diagnosis_data = None

    def create_healthcare_data(self):
        """Create synthetic healthcare dataset."""
        print("\n2. HEALTHCARE DATA SCIENCE")
        print("=" * 40)

        print("\n2.1 CREATING SYNTHETIC HEALTHCARE DATASET:")
        print("-" * 45)

        # Generate patient data
        n_patients = 800

        patient_data = {
            "patient_id": range(1, n_patients + 1),
            "age": np.random.normal(45, 18, n_patients).astype(int),
            "gender": np.random.choice(["Male", "Female"], n_patients),
            "bmi": np.random.normal(25, 5, n_patients),
            "blood_pressure_systolic": np.random.normal(120, 20, n_patients).astype(
                int
            ),
            "blood_pressure_diastolic": np.random.normal(80, 10, n_patients).astype(
                int
            ),
            "cholesterol": np.random.normal(200, 40, n_patients).astype(int),
            "glucose": np.random.normal(100, 25, n_patients).astype(int),
            "smoking_status": np.random.choice(
                ["Never", "Former", "Current"], n_patients, p=[0.6, 0.25, 0.15]
            ),
            "exercise_frequency": np.random.choice(
                ["None", "Low", "Moderate", "High"], n_patients, p=[0.2, 0.3, 0.3, 0.2]
            ),
        }

        self.patient_data = pd.DataFrame(patient_data)

        # Generate diagnosis data
        n_diagnoses = 1200
        diagnosis_data = {
            "diagnosis_id": range(1, n_diagnoses + 1),
            "patient_id": np.random.choice(range(1, n_patients + 1), n_diagnoses),
            "diagnosis_date": pd.date_range(
                "2023-01-01", periods=n_diagnoses, freq="D"
            ),
            "condition": np.random.choice(
                ["Diabetes", "Hypertension", "Heart Disease", "Obesity", "Healthy"],
                n_diagnoses,
                p=[0.15, 0.25, 0.20, 0.15, 0.25],
            ),
            "severity": np.random.choice(
                ["Mild", "Moderate", "Severe"], n_diagnoses, p=[0.5, 0.3, 0.2]
            ),
            "treatment_required": np.random.choice(
                [True, False], n_diagnoses, p=[0.7, 0.3]
            ),
        }

        self.diagnosis_data = pd.DataFrame(diagnosis_data)

        print(f"  ‚úÖ Patient dataset: {len(self.patient_data):,} patients")
        print(f"  ‚úÖ Diagnosis dataset: {len(self.diagnosis_data):,} diagnoses")
        print(
            f"  üìä Data spans: {self.diagnosis_data['diagnosis_date'].min()} to {self.diagnosis_data['diagnosis_date'].max()}"
        )

        return self.patient_data, self.diagnosis_data

    def disease_prediction(self):
        """Build disease prediction model."""
        print("\n2.2 DISEASE PREDICTION MODEL:")
        print("-" * 35)

        # Merge patient and diagnosis data
        merged_data = self.patient_data.merge(
            self.diagnosis_data.groupby("patient_id")["condition"]
            .first()
            .reset_index(),
            on="patient_id",
            how="left",
        )

        # Fill missing diagnoses with "Healthy"
        merged_data["condition"] = merged_data["condition"].fillna("Healthy")

        # Create binary target (any disease vs healthy)
        merged_data["has_disease"] = (merged_data["condition"] != "Healthy").astype(int)

        # Prepare features for disease prediction
        disease_features = [
            "age",
            "bmi",
            "blood_pressure_systolic",
            "blood_pressure_diastolic",
            "cholesterol",
            "glucose",
        ]
        X = merged_data[disease_features].copy()
        y = merged_data["has_disease"]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # Train disease prediction model
        disease_model = RandomForestClassifier(n_estimators=100, random_state=42)
        disease_model.fit(X_train, y_train)

        # Evaluate model
        y_pred = disease_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, disease_model.predict_proba(X_test)[:, 1])

        print(f"  üéØ Disease Prediction Results:")
        print(f"    Accuracy: {accuracy:.4f}")
        print(f"    AUC: {auc:.4f}")
        print(f"    Disease Rate: {y.mean():.1%}")

        # Feature importance
        feature_importance = pd.DataFrame(
            {
                "feature": disease_features,
                "importance": disease_model.feature_importances_,
            }
        ).sort_values("importance", ascending=False)

        print(f"\n  üîç Top Features for Disease Prediction:")
        for _, row in feature_importance.head(3).iterrows():
            print(f"    {row['feature']:25}: {row['importance']:.4f}")

        return disease_model, feature_importance


class FinancialCaseStudy:
    """Financial analytics and risk management case study."""

    def __init__(self):
        self.customer_data = None
        self.transaction_data = None
        self.credit_data = None

    def create_financial_data(self):
        """Create synthetic financial dataset."""
        print("\n3. FINANCIAL ANALYTICS AND RISK MANAGEMENT")
        print("=" * 55)

        print("\n3.1 CREATING SYNTHETIC FINANCIAL DATASET:")
        print("-" * 45)

        # Generate customer data
        n_customers = 600

        customer_data = {
            "customer_id": range(1, n_customers + 1),
            "age": np.random.normal(40, 15, n_customers).astype(int),
            "income": np.random.lognormal(10.8, 0.6, n_customers).astype(int),
            "employment_length": np.random.exponential(8, n_customers).astype(int),
            "credit_score": np.random.normal(650, 100, n_customers).astype(int),
            "debt_to_income_ratio": np.random.uniform(0.1, 0.8, n_customers),
            "number_of_accounts": np.random.poisson(5, n_customers),
            "has_bankruptcy": np.random.choice(
                [True, False], n_customers, p=[0.05, 0.95]
            ),
        }

        self.customer_data = pd.DataFrame(customer_data)

        # Generate transaction data
        n_transactions = 3000
        transaction_data = {
            "transaction_id": range(1, n_transactions + 1),
            "customer_id": np.random.choice(range(1, n_customers + 1), n_transactions),
            "transaction_type": np.random.choice(
                ["Purchase", "Withdrawal", "Transfer", "Payment"], n_transactions
            ),
            "amount": np.random.lognormal(3, 1.2, n_transactions),
            "date": pd.date_range("2023-01-01", periods=n_transactions, freq="H"),
            "is_fraudulent": np.random.choice(
                [True, False], n_transactions, p=[0.02, 0.98]
            ),
        }

        self.transaction_data = pd.DataFrame(transaction_data)

        # Generate credit data
        n_credit_applications = 400
        credit_data = {
            "application_id": range(1, n_credit_applications + 1),
            "customer_id": np.random.choice(
                range(1, n_customers + 1), n_credit_applications
            ),
            "loan_amount": np.random.lognormal(10, 0.8, n_credit_applications).astype(
                int
            ),
            "loan_purpose": np.random.choice(
                ["Home", "Car", "Business", "Personal"], n_credit_applications
            ),
            "loan_term": np.random.choice([12, 24, 36, 60, 84], n_credit_applications),
            "interest_rate": np.random.uniform(3.0, 18.0, n_credit_applications),
            "is_approved": np.random.choice(
                [True, False], n_credit_applications, p=[0.7, 0.3]
            ),
        }

        self.credit_data = pd.DataFrame(credit_data)

        print(f"  ‚úÖ Customer dataset: {len(self.customer_data):,} customers")
        print(f"  ‚úÖ Transaction dataset: {len(self.transaction_data):,} transactions")
        print(f"  ‚úÖ Credit applications: {len(self.credit_data):,} applications")

        return self.customer_data, self.transaction_data, self.credit_data

    def fraud_detection(self):
        """Build fraud detection model."""
        print("\n3.2 FRAUD DETECTION MODEL:")
        print("-" * 30)

        # Merge customer and transaction data
        merged_data = self.customer_data.merge(
            self.transaction_data, on="customer_id", how="inner"
        )

        # Create features for fraud detection
        fraud_features = [
            "age",
            "income",
            "credit_score",
            "debt_to_income_ratio",
            "number_of_accounts",
            "amount",
        ]
        X = merged_data[fraud_features].copy()
        y = merged_data["is_fraudulent"]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # Train fraud detection model
        fraud_model = RandomForestClassifier(n_estimators=100, random_state=42)
        fraud_model.fit(X_train, y_train)

        # Evaluate model
        y_pred = fraud_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, fraud_model.predict_proba(X_test)[:, 1])

        print(f"  üéØ Fraud Detection Results:")
        print(f"    Accuracy: {accuracy:.4f}")
        print(f"    AUC: {auc:.4f}")
        print(f"    Fraud Rate: {y.mean():.1%}")

        # Feature importance
        feature_importance = pd.DataFrame(
            {"feature": fraud_features, "importance": fraud_model.feature_importances_}
        ).sort_values("importance", ascending=False)

        print(f"\n  üîç Top Features for Fraud Detection:")
        for _, row in feature_importance.head(3).iterrows():
            print(f"    {row['feature']:25}: {row['importance']:.4f}")

        return fraud_model, feature_importance

    def credit_risk_assessment(self):
        """Build credit risk assessment model."""
        print("\n3.3 CREDIT RISK ASSESSMENT MODEL:")
        print("-" * 40)

        # Merge customer and credit data
        merged_data = self.customer_data.merge(
            self.credit_data, on="customer_id", how="inner"
        )

        # Create features for credit risk
        risk_features = [
            "age",
            "income",
            "employment_length",
            "credit_score",
            "debt_to_income_ratio",
            "number_of_accounts",
            "loan_amount",
            "loan_term",
            "interest_rate",
        ]
        X = merged_data[risk_features].copy()
        y = merged_data["is_approved"]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        # Train credit risk model
        risk_model = RandomForestClassifier(n_estimators=100, random_state=42)
        risk_model.fit(X_train, y_train)

        # Evaluate model
        y_pred = risk_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, risk_model.predict_proba(X_test)[:, 1])

        print(f"  üéØ Credit Risk Assessment Results:")
        print(f"    Accuracy: {accuracy:.4f}")
        print(f"    AUC: {auc:.4f}")
        print(f"    Approval Rate: {y.mean():.1%}")

        # Feature importance
        feature_importance = pd.DataFrame(
            {"feature": risk_features, "importance": risk_model.feature_importances_}
        ).sort_values("importance", ascending=False)

        print(f"\n  üîç Top Features for Credit Risk Assessment:")
        for _, row in feature_importance.head(3).iterrows():
            print(f"    {row['feature']:25}: {row['importance']:.4f}")

        return risk_model, feature_importance


def create_case_study_visualizations():
    """Create comprehensive visualizations for all case studies."""
    print("\n4. CREATING CASE STUDY VISUALIZATIONS:")
    print("-" * 45)

    print("  Generating visualizations...")

    # Create figure with subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle(
        "Real-World Case Studies: Industry Applications Overview",
        fontsize=16,
        fontweight="bold",
    )

    # 1. E-Commerce Customer Segments
    ax1 = axes[0, 0]
    segments = [
        "High-Value Loyal",
        "Budget Conscious",
        "Occasional Buyers",
        "Premium Customers",
    ]
    segment_sizes = [250, 300, 200, 250]  # Simulated segment sizes

    bars = ax1.pie(segment_sizes, labels=segments, autopct="%1.1f%%", startangle=90)
    ax1.set_title("E-Commerce Customer Segments", fontweight="bold")

    # 2. Healthcare Disease Distribution
    ax2 = axes[0, 1]
    conditions = ["Diabetes", "Hypertension", "Heart Disease", "Obesity", "Healthy"]
    condition_counts = [120, 200, 160, 120, 200]  # Simulated counts

    bars = ax2.bar(
        conditions,
        condition_counts,
        color=["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FFA07A"],
        alpha=0.8,
    )
    ax2.set_title("Healthcare Condition Distribution", fontweight="bold")
    ax2.set_ylabel("Number of Patients")
    ax2.tick_params(axis="x", rotation=45)

    # 3. Financial Fraud Detection
    ax3 = axes[0, 2]
    fraud_types = ["Legitimate", "Fraudulent"]
    fraud_counts = [2940, 60]  # 2% fraud rate

    bars = ax3.bar(fraud_types, fraud_counts, color=["#98D8C8", "#FF6B6B"], alpha=0.8)
    ax3.set_title("Financial Transaction Fraud Detection", fontweight="bold")
    ax3.set_ylabel("Number of Transactions")

    # Add value labels
    for bar, count in zip(bars, fraud_counts):
        height = bar.get_height()
        ax3.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 20,
            f"{count:,}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 4. Customer Churn Analysis
    ax4 = axes[1, 0]
    churn_status = ["Retained", "Churned"]
    churn_counts = [700, 300]  # Simulated churn rate

    bars = ax4.bar(churn_status, churn_counts, color=["#4ECDC4", "#FF6B6B"], alpha=0.8)
    ax4.set_title("Customer Churn Analysis", fontweight="bold")
    ax4.set_ylabel("Number of Customers")

    # Add value labels
    for bar, count in zip(bars, churn_counts):
        height = bar.get_height()
        ax4.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 20,
            f"{count:,}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 5. Credit Risk Assessment
    ax5 = axes[1, 1]
    risk_status = ["Approved", "Rejected"]
    risk_counts = [280, 120]  # 70% approval rate

    bars = ax5.bar(risk_status, risk_counts, color=["#98D8C8", "#FFA07A"], alpha=0.8)
    ax5.set_title("Credit Risk Assessment", fontweight="bold")
    ax5.set_ylabel("Number of Applications")

    # Add value labels
    for bar, count in zip(bars, risk_counts):
        height = bar.get_height()
        ax5.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 20,
            f"{count:,}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 6. Model Performance Comparison
    ax6 = axes[1, 2]
    models = [
        "Churn Prediction",
        "Disease Prediction",
        "Fraud Detection",
        "Credit Risk",
    ]
    accuracies = [0.89, 0.92, 0.95, 0.87]  # Simulated accuracies

    bars = ax6.barh(
        models,
        accuracies,
        color=["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4"],
        alpha=0.8,
    )
    ax6.set_title("Model Performance Comparison", fontweight="bold")
    ax6.set_xlabel("Accuracy")
    ax6.set_xlim(0.8, 1.0)

    # Add value labels
    for bar, acc in zip(bars, accuracies):
        width = bar.get_width()
        ax6.text(
            width + 0.01,
            bar.get_y() + bar.get_height() / 2.0,
            f"{acc:.2f}",
            ha="left",
            va="center",
            fontweight="bold",
        )

    plt.tight_layout()

    # Save visualization
    output_file = "real_world_case_studies.png"
    plt.savefig(output_file, dpi=300, bbox_inches="tight")
    print(f"  ‚úÖ Visualization saved: {output_file}")

    plt.show()


def main():
    """Main function to run case studies."""
    try:
        print("=" * 80)
        print("CHAPTER 19: REAL-WORLD CASE STUDIES")
        print("=" * 80)

        # Initialize case studies
        ecommerce = ECommerceCaseStudy()
        healthcare = HealthcareCaseStudy()
        financial = FinancialCaseStudy()

        # Run e-commerce case study
        print("\n" + "=" * 80)
        customer_data, transaction_data = ecommerce.create_ecommerce_data()
        segmented_data = ecommerce.customer_segmentation()
        churn_model, churn_features = ecommerce.churn_prediction()

        # Run healthcare case study
        print("\n" + "=" * 80)
        patient_data, diagnosis_data = healthcare.create_healthcare_data()
        disease_model, disease_features = healthcare.disease_prediction()

        # Run financial case study
        print("\n" + "=" * 80)
        fin_customer_data, fin_transaction_data, credit_data = (
            financial.create_financial_data()
        )
        fraud_model, fraud_features = financial.fraud_detection()
        risk_model, risk_features = financial.credit_risk_assessment()

        # Create visualizations
        create_case_study_visualizations()

        print("\n" + "=" * 80)
        print("CHAPTER 19 - ALL CASE STUDIES COMPLETED!")
        print("=" * 80)
        print("\nüéØ What You've Learned:")
        print("  ‚Ä¢ E-Commerce: Customer segmentation and churn prediction")
        print("  ‚Ä¢ Healthcare: Disease prediction and patient risk assessment")
        print("  ‚Ä¢ Financial: Fraud detection and credit risk modeling")
        print("  ‚Ä¢ End-to-end data science solutions for real-world problems")

        print("\nüìä Generated Visualizations:")
        print("  ‚Ä¢ real_world_case_studies.png - Comprehensive industry overview")

        print("\nüöÄ Next Steps:")
        print("  ‚Ä¢ Apply these techniques to your own datasets")
        print("  ‚Ä¢ Build portfolio projects from these case studies")
        print("  ‚Ä¢ Continue to Chapter 20: Data Science Ethics")

    except Exception as e:
        print(f"\n‚ùå Error in Chapter 19: {str(e)}")
        print("Please check the error and try again.")


if __name__ == "__main__":
    main()
